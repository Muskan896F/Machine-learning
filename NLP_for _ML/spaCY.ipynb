{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Functions Explained\n",
    "\n",
    "## Introduction to spaCy\n",
    "spaCy is an open-source library for advanced natural language processing in Python. It is designed for efficiency and ease of use, providing pre-trained models for various languages and supporting many NLP tasks.\n",
    "\n",
    "## 1. Tokenization\n",
    "Tokenization in spaCy involves breaking text into smaller units (tokens), which can be words or sentences.\n",
    "\n",
    "### **1.1 Word Tokenization**\n",
    "- **Function**: `nlp()`\n",
    "- **Syntax**: `nlp(text)`\n",
    "- **Description**: Processes a given text and breaks it down into tokens.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    import spacy\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    text = \"Natural language processing with spaCy is efficient!\"\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc]\n",
    "    print(words)  # Output: ['Natural', 'language', 'processing', 'with', 'spaCy', 'is', 'efficient', '!']\n",
    "    ```\n",
    "\n",
    "### **1.2 Sentence Tokenization**\n",
    "- **Function**: `nlp()`\n",
    "- **Syntax**: `nlp(text)`\n",
    "- **Description**: Processes the text and segments it into sentences.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    doc = nlp(\"Hello world. How are you?\")\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    print(sentences)  # Output: ['Hello world.', 'How are you?']\n",
    "    ```\n",
    "\n",
    "## 2. Stop Words Removal\n",
    "spaCy includes a built-in list of stop words that can be easily filtered out.\n",
    "\n",
    "- **Function**: `is_stop`\n",
    "- **Syntax**: `token.is_stop`\n",
    "- **Description**: Checks if a token is a stop word.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    doc = nlp(\"The cat sat on the mat.\")\n",
    "    stop_words = [token.text for token in doc if token.is_stop]\n",
    "    print(stop_words)  # Output: ['The', 'on', 'the']\n",
    "    ```\n",
    "\n",
    "## 3. Stemming and Lemmatization\n",
    "spaCy primarily focuses on lemmatization rather than stemming.\n",
    "\n",
    "- **Function**: `lemma_`\n",
    "- **Syntax**: `token.lemma_`\n",
    "- **Description**: Provides the lemma (base form) of a word.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    doc = nlp(\"better\")\n",
    "    lemmatized_word = [token.lemma_ for token in doc]\n",
    "    print(lemmatized_word)  # Output: ['good']\n",
    "    ```\n",
    "\n",
    "## 4. Part-of-Speech Tagging (POS)\n",
    "spaCy performs part-of-speech tagging to identify the grammatical categories of tokens.\n",
    "\n",
    "- **Function**: `tag_`\n",
    "- **Syntax**: `token.tag_`\n",
    "- **Description**: Provides the part-of-speech tag for each token.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    doc = nlp(\"Python is great\")\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    print(pos_tags)  # Output: [('Python', 'PROPN'), ('is', 'AUX'), ('great', 'ADJ')]\n",
    "    ```\n",
    "\n",
    "## 5. Named Entity Recognition (NER)\n",
    "spaCy has robust named entity recognition capabilities.\n",
    "\n",
    "- **Function**: `ents`\n",
    "- **Syntax**: `doc.ents`\n",
    "- **Description**: Extracts named entities from the processed document.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    print(entities)  # Output: [('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n",
    "    ```\n",
    "\n",
    "## 6. Dependency Parsing\n",
    "Dependency parsing identifies the grammatical structure of a sentence by establishing relationships between words.\n",
    "\n",
    "- **Function**: `dep_`\n",
    "- **Syntax**: `token.dep_`\n",
    "- **Description**: Provides the syntactic dependency relation of a token.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "    dependencies = [(token.text, token.dep_) for token in doc]\n",
    "    print(dependencies)  # Output: [('The', 'det'), ('quick', 'amod'), ('brown', 'amod'), ('fox', 'nsubj'), ('jumps', 'ROOT'), ...]\n",
    "    ```\n",
    "\n",
    "## 7. Text Classification\n",
    "spaCy supports text classification through custom training.\n",
    "\n",
    "- **Function**: `TextCategorizer`\n",
    "- **Syntax**: `nlp.add_pipe(\"textcat\", config={...})`\n",
    "- **Description**: Allows for adding a text categorization pipeline.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from spacy.pipeline.textcat import Config\n",
    "\n",
    "    config = Config().from_str('{\"model\": \"textcat\"}')\n",
    "    nlp.add_pipe(\"textcat\", config=config)\n",
    "    # Continue with model training as required\n",
    "    ```\n",
    "\n",
    "## 8. Corpora Access\n",
    "spaCy does not provide direct access to corpora like NLTK, but it can be integrated with datasets.\n",
    "\n",
    "## 9. Custom Pipeline Components\n",
    "spaCy allows for custom components in the NLP pipeline.\n",
    "\n",
    "- **Function**: `nlp.add_pipe()`\n",
    "- **Syntax**: `nlp.add_pipe(component, name='component_name')`\n",
    "- **Description**: Adds a custom component to the processing pipeline.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    def custom_component(doc):\n",
    "        print(\"Custom component processing text...\")\n",
    "        return doc\n",
    "\n",
    "    nlp.add_pipe(custom_component, last=True)\n",
    "    doc = nlp(\"This will trigger the custom component.\")\n",
    "    ```\n",
    "\n",
    "## 10. Visualizing Dependency Parse\n",
    "spaCy can visualize dependency parsing with its integrated visualization tools.\n",
    "\n",
    "- **Function**: `displacy.render()`\n",
    "- **Syntax**: `displacy.render(doc, style='dep')`\n",
    "- **Description**: Renders the dependency tree of a document.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from spacy import displacy\n",
    "\n",
    "    doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "    displacy.render(doc, style='dep', jupyter=True)  # Displays in Jupyter Notebook\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Functions Explained\n",
    "\n",
    "## Introduction to spaCy\n",
    "spaCy is an open-source library for advanced natural language processing in Python. It is designed for efficiency and ease of use, providing pre-trained models for various languages and supporting many NLP tasks.\n",
    "\n",
    "## Table of spaCy Functions\n",
    "\n",
    "| **Function**                       | **Syntax**                          | **Description**                                                        | **Example**                                                                                                                                                       |\n",
    "|------------------------------------|-------------------------------------|------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Tokenization**                   | `nlp(text)`                        | Processes a given text and breaks it down into tokens.                | ```python<br>import spacy<br>nlp = spacy.load(\"en_core_web_sm\")<br>text = \"Natural language processing with spaCy is efficient!\"<br>doc = nlp(text)<br>words = [token.text for token in doc]<br>print(words)  # Output: ['Natural', 'language', 'processing', 'with', 'spaCy', 'is', 'efficient', '!']``` |\n",
    "| **Sentence Tokenization**          | `nlp(text)`                        | Segments the text into sentences.                                     | ```python<br>doc = nlp(\"Hello world. How are you?\")<br>sentences = [sent.text for sent in doc.sents]<br>print(sentences)  # Output: ['Hello world.', 'How are you?']```                         |\n",
    "| **Stop Words Removal**             | `token.is_stop`                   | Checks if a token is a stop word.                                     | ```python<br>doc = nlp(\"The cat sat on the mat.\")<br>stop_words = [token.text for token in doc if token.is_stop]<br>print(stop_words)  # Output: ['The', 'on', 'the']```                            |\n",
    "| **Lemmatization**                  | `token.lemma_`                    | Provides the lemma (base form) of a word.                             | ```python<br>doc = nlp(\"better\")<br>lemmatized_word = [token.lemma_ for token in doc]<br>print(lemmatized_word)  # Output: ['good']```                                                   |\n",
    "| **Part-of-Speech Tagging (POS)**   | `token.tag_`                      | Provides the part-of-speech tag for each token.                       | ```python<br>doc = nlp(\"Python is great\")<br>pos_tags = [(token.text, token.pos_) for token in doc]<br>print(pos_tags)  # Output: [('Python', 'PROPN'), ('is', 'AUX'), ('great', 'ADJ')]```        |\n",
    "| **Named Entity Recognition (NER)**  | `doc.ents`                        | Extracts named entities from the processed document.                  | ```python<br>doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")<br>entities = [(ent.text, ent.label_) for ent in doc.ents]<br>print(entities)  # Output: [('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]``` |\n",
    "| **Dependency Parsing**             | `token.dep_`                      | Provides the syntactic dependency relation of a token.                | ```python<br>doc = nlp(\"The quick brown fox jumps over the lazy dog.\")<br>dependencies = [(token.text, token.dep_) for token in doc]<br>print(dependencies)  # Output: [('The', 'det'), ('quick', 'amod'), ('brown', 'amod'), ('fox', 'nsubj'), ('jumps', 'ROOT'), ...]``` |\n",
    "| **Text Classification**            | `nlp.add_pipe(\"textcat\", config={...})` | Allows for adding a text categorization pipeline.                     | ```python<br>from spacy.pipeline.textcat import Config<br>config = Config().from_str('{\"model\": \"textcat\"}')<br>nlp.add_pipe(\"textcat\", config=config)<br># Continue with model training as required``` |\n",
    "| **Custom Pipeline Components**      | `nlp.add_pipe(component, name='component_name')` | Adds a custom component to the processing pipeline.                   | ```python<br>def custom_component(doc):<br>    print(\"Custom component processing text...\")<br>    return doc<br>nlp.add_pipe(custom_component, last=True)<br>doc = nlp(\"This will trigger the custom component.\")``` |\n",
    "| **Visualizing Dependency Parse**   | `displacy.render(doc, style='dep')` | Renders the dependency tree of a document.                            | ```python<br>from spacy import displacy<br>doc = nlp(\"The quick brown fox jumps over the lazy dog.\")<br>displacy.render(doc, style='dep', jupyter=True)  # Displays in Jupyter Notebook``` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of spaCy and NLTK\n",
    "\n",
    "## Overview\n",
    "Both spaCy and NLTK are popular libraries for Natural Language Processing (NLP) in Python, but they have different design philosophies and functionalities.\n",
    "\n",
    "| **Feature**                   | **spaCy**                                                 | **NLTK**                                                  |\n",
    "|-------------------------------|----------------------------------------------------------|----------------------------------------------------------|\n",
    "| **Purpose**                   | Designed for production and efficiency in NLP tasks.     | More of an educational tool with a wide variety of NLP tasks. |\n",
    "| **Ease of Use**               | User-friendly API, straightforward and intuitive.       | Slightly steeper learning curve due to its complexity.    |\n",
    "| **Speed**                     | Optimized for performance, faster in processing.         | Generally slower; offers more flexibility and options.     |\n",
    "| **Pre-trained Models**        | Provides state-of-the-art pre-trained models for various languages. | Limited pre-trained models; focuses on educational resources. |\n",
    "| **Tokenization**              | Advanced tokenization that accounts for various cases.   | Basic tokenization; can be less efficient for large texts.  |\n",
    "| **Lemmatization/Stemming**    | Integrated lemmatization; stemming available but less emphasized. | Both stemming and lemmatization options available.          |\n",
    "| **Named Entity Recognition**   | Built-in NER with high accuracy.                         | NER functionality is available but less efficient.         |\n",
    "| **Dependency Parsing**        | Powerful dependency parsing features included.           | Dependency parsing available but less intuitive.           |\n",
    "| **Visualization**             | Offers visualization tools (e.g., displaCy) for NER and dependencies. | Visualization requires additional tools (e.g., Matplotlib). |\n",
    "| **Community and Support**     | Growing community, well-documented with active support.  | Established community with extensive documentation and resources. |\n",
    "| **Text Classification**       | Integrated text classification support with pipelines.   | Requires more setup for text classification tasks.         |\n",
    "| **Corpora**                   | Limited built-in corpora; focuses on model training.     | Extensive collection of corpora and linguistic datasets.    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of spaCy and NLTK Syntax\n",
    "\n",
    "## Overview\n",
    "Both spaCy and NLTK offer unique syntaxes and functionalities for natural language processing, catering to different use cases. Below is a comparison highlighting their syntax for various NLP tasks.\n",
    "\n",
    "| **Task**                        | **spaCy Syntax**                                       | **NLTK Syntax**                                           |\n",
    "|---------------------------------|-------------------------------------------------------|----------------------------------------------------------|\n",
    "| **Importing Library**           | ```python<br>import spacy<br>nlp = spacy.load(\"en_core_web_sm\")``` | ```python<br>import nltk<br>nltk.download('punkt')```     |\n",
    "| **Tokenization**                | ```python<br>doc = nlp(\"Text to tokenize\")<br>tokens = [token.text for token in doc]``` | ```python<br>from nltk.tokenize import word_tokenize<br>tokens = word_tokenize(\"Text to tokenize\")``` |\n",
    "| **Sentence Tokenization**       | ```python<br>sentences = [sent.text for sent in doc.sents]``` | ```python<br>from nltk.tokenize import sent_tokenize<br>sentences = sent_tokenize(\"Text to tokenize.\")``` |\n",
    "| **Stop Words Removal**          | ```python<br>stop_words = [token.text for token in doc if token.is_stop]``` | ```python<br>from nltk.corpus import stopwords<br>stop_words = set(stopwords.words('english'))``` |\n",
    "| **Lemmatization**               | ```python<br>lemmatized_word = [token.lemma_ for token in doc]``` | ```python<br>from nltk.stem import WordNetLemmatizer<br>lemmatizer = WordNetLemmatizer()<br>lemmatized_word = lemmatizer.lemmatize(\"better\", pos='a')``` |\n",
    "| **Stemming**                    | ```python<br>from spacy.lang.en import English<br>nlp = English()<br>stemmed_word = nlp(\"running\")[0].lemma_``` | ```python<br>from nltk.stem import PorterStemmer<br>ps = PorterStemmer()<br>stemmed_word = ps.stem(\"running\")``` |\n",
    "| **Part-of-Speech Tagging (POS)**| ```python<br>pos_tags = [(token.text, token.pos_) for token in doc]``` | ```python<br>from nltk import pos_tag<br>pos_tags = pos_tag(tokens)``` |\n",
    "| **Named Entity Recognition (NER)**| ```python<br>entities = [(ent.text, ent.label_) for ent in doc.ents]``` | ```python<br>from nltk import ne_chunk<br>ne_tree = ne_chunk(pos_tags)``` |\n",
    "| **Dependency Parsing**          | ```python<br>dependencies = [(token.text, token.dep_) for token in doc]``` | ```python<br>from nltk.parse import CoreNLPParser<br>parser = CoreNLPParser()<br>parsed_sentence = list(parser.raw_parse(\"The cat sat on the mat.\"))``` |\n",
    "| **Text Classification**         | ```python<br>nlp.add_pipe(\"textcat\")<br>text_cat = nlp.get_pipe(\"textcat\")``` | ```python<br>from nltk.classify import NaiveBayesClassifier<br>classifier = NaiveBayesClassifier.train(train_data)``` |\n",
    "| **Corpora Access**              | ```python<br>from spacy.cli import download<br>download(\"en_core_web_sm\")``` | ```python<br>from nltk.corpus import movie_reviews<br>words = movie_reviews.words()``` |\n",
    "| **Visualizing Dependency Parse**| ```python<br>from spacy import displacy<br>displacy.render(doc, style='dep')``` | ```python<br>import matplotlib.pyplot as plt<br># Use additional libraries for visualization``` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The End\n"
     ]
    }
   ],
   "source": [
    "print(\"The End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
