{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK (Natural Language Toolkit) in Python\n",
    "\n",
    "NLTK, or the Natural Language Toolkit, is one of the most popular libraries for processing and analyzing human language data (text). It provides a wide range of functionalities for tasks such as tokenization, stemming, lemmatization, part-of-speech tagging, and more.\n",
    "\n",
    "## Key Features of NLTK\n",
    "- **Tokenization**: Splitting text into sentences or words.\n",
    "- **Stemming & Lemmatization**: Reducing words to their base or root form.\n",
    "- **Part-of-Speech Tagging (POS)**: Labeling words with their respective parts of speech (e.g., nouns, verbs).\n",
    "- **Named Entity Recognition (NER)**: Identifying entities like names, places, organizations.\n",
    "- **Stop Words Removal**: Removing common words (e.g., \"the\", \"is\") that may not be useful for analysis.\n",
    "- **Parsing**: Analyzing the grammatical structure of sentences.\n",
    "- **Text Classification**: Categorizing text into predefined categories (e.g., spam vs. not spam).\n",
    "- **Corpora**: Access to numerous linguistic datasets, such as movie reviews, news articles, and dictionaries.\n",
    "\n",
    "## Installation\n",
    "\n",
    "To install NLTK, you can use `pip`:\n",
    "\n",
    "```bash\n",
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\md199\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\md199\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\md199\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\md199\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\md199\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\md199\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Functionalities of NLTK\n",
    "\n",
    "### 1. **Tokenization**\n",
    "Tokenization is the process of splitting text into smaller units, such as words or sentences.\n",
    "\n",
    "- **Word Tokenization**: Splits text into words.\n",
    "- **Sentence Tokenization**: Splits text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization: ['Natural', 'language', 'processing', 'with', 'NLTK', 'is', 'interesting', '!', 'Let', \"'s\", 'learn', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural language processing with NLTK is interesting! Let's learn.\"\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"Word Tokenization:\", words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: ['Natural language processing with NLTK is interesting!', \"Let's learn.\"]\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokenization:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Stop Words Removal**\n",
    "Stop words are common words that usually do not carry much meaning, such as \"the\", \"is\", \"in\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['Natural', 'language', 'processing', 'NLTK', 'interesting', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural language processing with NLTK is interesting!\"\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Filtering stop words\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(\"Filtered Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Stemming**\n",
    "Stemming reduces words to their base form (e.g., \"running\" becomes \"run\"). NLTK provides several stemming algorithms, including the Porter Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['natur', 'languag', 'process', 'with', 'nltk', 'is', 'interest', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Stemming words\n",
    "stemmed_words = [ps.stem(word) for word in words]\n",
    "print(\"Stemmed Words:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Lemmatization**\n",
    "Lemmatization reduces words to their lemma (base form), but it uses a vocabulary and part-of-speech tagging to determine the correct base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\md199/nltk_data'\n    - 'c:\\\\Users\\\\md199\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\md199\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\md199\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\md199\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\md199\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\md199\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\md199/nltk_data'\n    - 'c:\\\\Users\\\\md199\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\md199\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\md199\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\md199\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Lemmatize words\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m lemmatized_words \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLemmatized Words:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lemmatized_words)\n",
      "File \u001b[1;32mc:\\Users\\md199\\anaconda3\\Lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32mc:\\Users\\md199\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\md199\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\md199\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\md199\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\md199/nltk_data'\n    - 'c:\\\\Users\\\\md199\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\md199\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\md199\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\md199\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize words\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Part-of-Speech Tagging (POS)**\n",
    "Part-of-speech tagging labels words with their parts of speech (e.g., noun, verb, adjective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\vikas/nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\vikas\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pos_tag\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# POS tagging\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m pos_tags \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOS Tags:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pos_tags)\n",
      "File \u001b[1;32mc:\\Users\\vikas\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\vikas\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\vikas\\anaconda3\\Lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPICKLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32mc:\\Users\\vikas\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\vikas/nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\vikas\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "# POS tagging\n",
    "pos_tags = pos_tag(words)\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. **Named Entity Recognition (NER)**\n",
    "Named Entity Recognition identifies and classifies entities in text, such as names, locations, organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ne_chunk\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Perform NER\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m ner_tree \u001b[38;5;241m=\u001b[39m ne_chunk(\u001b[43mpos_tags\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(ner_tree)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pos_tags' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "# Perform NER\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "print(ner_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. **Text Classification**\n",
    "NLTK provides tools for building text classifiers, including support for Naive Bayes, Decision Trees, etc. Here’s a simple Naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Accuracy: 1.0\n",
      "Classify 'good': positive\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "\n",
    "# Define some training data (features and labels)\n",
    "train_data = [({'word': 'good'}, 'positive'),\n",
    "              ({'word': 'bad'}, 'negative')]\n",
    "\n",
    "# Train the classifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "# Test classifier accuracy\n",
    "print(\"Classifier Accuracy:\", accuracy(classifier, train_data))\n",
    "\n",
    "# Classify a new instance\n",
    "print(\"Classify 'good':\", classifier.classify({'word': 'good'}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. **Corpora Access**\n",
    "NLTK provides access to various linguistic datasets, or corpora. To use them, you need to download the corpus data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mmovie_reviews\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('movie_reviews')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/movie_reviews\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\vikas/nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\vikas\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\vikas\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\vikas\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mmovie_reviews\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('movie_reviews')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/movie_reviews.zip/movie_reviews/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\vikas/nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\vikas\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Access movie reviews corpus\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m movie_reviews\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmovie_reviews\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m()[:\u001b[38;5;241m20\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\vikas\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\vikas\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\vikas\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\vikas\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mmovie_reviews\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('movie_reviews')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/movie_reviews\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\vikas/nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\vikas\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\vikas\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Access movie reviews corpus\n",
    "from nltk.corpus import movie_reviews\n",
    "print(movie_reviews.words()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. **Text Parsing**\n",
    "Parsing analyzes the grammatical structure of a sentence. NLTK provides a CFG (Context-Free Grammar) parser for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (Det the) (N dog)) (VP (V chased) (NP (Det the) (N cat))))\n"
     ]
    }
   ],
   "source": [
    "from nltk import CFG\n",
    "from nltk.parse import RecursiveDescentParser\n",
    "\n",
    "# Define grammar rules\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  NP -> Det N\n",
    "  VP -> V NP\n",
    "  Det -> 'the'\n",
    "  N -> 'dog' | 'cat'\n",
    "  V -> 'chased' | 'sat'\n",
    "\"\"\")\n",
    "\n",
    "# Initialize parser\n",
    "parser = RecursiveDescentParser(grammar)\n",
    "\n",
    "# Parse sentence\n",
    "for tree in parser.parse(\"the dog chased the cat\".split()):\n",
    "    print(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. **Additional Features**\n",
    "* **Synonym and Antonym Lookup**: Using the WordNet lexical database, you can find synonyms and antonyms of words.\n",
    "* **Collocations**: Finding phrases or words that frequently appear together in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms of good: ['good', 'good', 'good', 'commodity', 'good', 'full', 'good', 'estimable', 'beneficial', 'good', 'good', 'adept', 'good', 'dear', 'dependable', 'good', 'good', 'effective', 'good', 'good', 'good', 'good', 'good', 'good', 'good', 'well', 'thoroughly']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Synonyms for \"good\"\n",
    "synonyms = wordnet.synsets(\"good\")\n",
    "print(\"Synonyms of good:\", [syn.lemmas()[0].name() for syn in synonyms])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NLTK Resources**\n",
    "**Corpora and Data Sources**\n",
    "* **Corpora:** Predefined datasets, including movie reviews, news articles, Shakespearean texts, etc.\n",
    "\n",
    "* **WordNet:** A lexical database for English, useful for word relationships like synonyms and antonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization**\n",
    "NLTK also provides basic tools for visualizing language structures, such as parse trees and frequency distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Frequency distribution of words\n",
    "fdist = FreqDist(words)\n",
    "fdist.plot(30, cumulative=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Functions Explained\n",
    "\n",
    "## 1. Tokenization\n",
    "Tokenization is the process of splitting text into smaller units (tokens), such as words or sentences.\n",
    "\n",
    "### **1.1 Word Tokenization**\n",
    "- **Function**: `word_tokenize()`\n",
    "- **Syntax**: `word_tokenize(text)`\n",
    "- **Description**: Splits a given text into a list of words.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    text = \"Natural language processing with NLTK is interesting!\"\n",
    "    words = word_tokenize(text)\n",
    "    print(words)  # Output: ['Natural', 'language', 'processing', 'with', 'NLTK', 'is', 'interesting', '!']\n",
    "    ```\n",
    "\n",
    "### **1.2 Sentence Tokenization**\n",
    "- **Function**: `sent_tokenize()`\n",
    "- **Syntax**: `sent_tokenize(text)`\n",
    "- **Description**: Splits a given text into a list of sentences.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "    text = \"Hello world. How are you?\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(sentences)  # Output: ['Hello world.', 'How are you?']\n",
    "    ```\n",
    "\n",
    "## 2. Stop Words Removal\n",
    "Stop words are common words that may not carry significant meaning in a given context.\n",
    "\n",
    "- **Function**: `stopwords.words()`\n",
    "- **Syntax**: `stopwords.words('language')`\n",
    "- **Description**: Retrieves a list of stop words for a specified language.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    print(stop_words)  # Output: set of common English stop words\n",
    "    ```\n",
    "\n",
    "## 3. Stemming\n",
    "Stemming reduces words to their root form.\n",
    "\n",
    "- **Function**: `PorterStemmer()`\n",
    "- **Syntax**: `PorterStemmer().stem(word)`\n",
    "- **Description**: Applies the Porter stemming algorithm to reduce words to their stems.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from nltk.stem import PorterStemmer\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_word = ps.stem(\"running\")\n",
    "    print(stemmed_word)  # Output: 'run'\n",
    "    ```\n",
    "\n",
    "## 4. Lemmatization\n",
    "Lemmatization reduces words to their base or dictionary form, considering context.\n",
    "\n",
    "- **Function**: `WordNetLemmatizer()`\n",
    "- **Syntax**: `WordNetLemmatizer().lemmatize(word, pos='n')`\n",
    "- **Description**: Uses WordNet to find the lemma of a word.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_word = lemmatizer.lemmatize(\"better\", pos='a')  # pos='a' for adjective\n",
    "    print(lemmatized_word)  # Output: 'good'\n",
    "    ```\n",
    "\n",
    "## 5. Part-of-Speech Tagging (POS)\n",
    "POS tagging assigns parts of speech to each word in a sentence.\n",
    "\n",
    "- **Function**: `pos_tag()`\n",
    "- **Syntax**: `pos_tag(tokens)`\n",
    "- **Description**: Tags a list of words with their corresponding parts of speech.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from nltk import pos_tag\n",
    "\n",
    "    tokens = word_tokenize(\"Python is fun\")\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    print(pos_tags)  # Output: [('Python', 'NNP'), ('is', 'VBZ'), ('fun', 'NN')]\n",
    "    ```\n",
    "\n",
    "## 6. Named Entity Recognition (NER)\n",
    "NER identifies and classifies entities in text, such as names of people or organizations.\n",
    "\n",
    "- **Function**: `ne_chunk()`\n",
    "- **Syntax**: `ne_chunk(pos_tags)`\n",
    "- **Description**: Analyzes a sequence of POS-tagged words and returns a tree of named entities.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from nltk import ne_chunk\n",
    "\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    ner_tree = ne_chunk(pos_tags)\n",
    "    print(ner_tree)  # Output: Tree with named entities\n",
    "    ```\n",
    "\n",
    "## 7. Text Classification\n",
    "Text classification involves categorizing text into predefined categories.\n",
    "\n",
    "- **Function**: `NaiveBayesClassifier.train()`\n",
    "- **Syntax**: `NaiveBayesClassifier.train(train_data)`\n",
    "- **Description**: Trains a Naive Bayes classifier using labeled training data.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "    train_data = [({'word': 'happy'}, 'positive'),\n",
    "                  ({'word': 'sad'}, 'negative')]\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "    print(classifier.classify({'word': 'happy'}))  # Output: 'positive'\n",
    "    ```\n",
    "\n",
    "## 8. Corpora Access\n",
    "NLTK provides access to a variety of linguistic datasets.\n",
    "\n",
    "- **Function**: `nltk.corpus.corpora_name`\n",
    "- **Syntax**: `nltk.corpus.corpora_name.words()`\n",
    "- **Description**: Allows access to specific corpora.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from nltk.corpus import movie_reviews\n",
    "\n",
    "    print(movie_reviews.words()[:10])  # Output: First 10 words in the movie reviews corpus\n",
    "    ```\n",
    "\n",
    "## 9. Text Parsing\n",
    "Parsing analyzes the grammatical structure of a sentence.\n",
    "\n",
    "- **Function**: `RecursiveDescentParser()`\n",
    "- **Syntax**: `RecursiveDescentParser(grammar)`\n",
    "- **Description**: Parses sentences based on defined grammar rules.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from nltk import CFG, RecursiveDescentParser\n",
    "\n",
    "    grammar = CFG.fromstring(\"\"\"\n",
    "      S -> NP VP\n",
    "      NP -> Det N\n",
    "      VP -> V NP\n",
    "      Det -> 'the'\n",
    "      N -> 'dog' | 'cat'\n",
    "      V -> 'chased' | 'sat'\n",
    "    \"\"\")\n",
    "\n",
    "    parser = RecursiveDescentParser(grammar)\n",
    "    for tree in parser.parse(\"the dog chased the cat\".split()):\n",
    "        print(tree)  # Output: Parse tree of the sentence\n",
    "    ```\n",
    "\n",
    "## 10. Frequency Distribution\n",
    "Frequency distribution provides insights into the frequency of tokens.\n",
    "\n",
    "- **Function**: `FreqDist()`\n",
    "- **Syntax**: `FreqDist(tokens)`\n",
    "- **Description**: Computes the frequency distribution of tokens.\n",
    "- **Example**:\n",
    "    ```python\n",
    "    from nltk import FreqDist\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    words = [\"dog\", \"cat\", \"dog\", \"cat\", \"cat\", \"mouse\"]\n",
    "    fdist = FreqDist(words)\n",
    "    fdist.plot(30, cumulative=False)  # Plot frequency distribution\n",
    "    plt.show()\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Functionality**                    | **Function**                          | **Syntax**                                     | **Description**                                                                         | **Example**                                                                                                                   |\n",
    "|--------------------------------------|--------------------------------------|------------------------------------------------|-----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Tokenization**                     | Word Tokenization                    | `word_tokenize(text)`                          | Splits text into a list of words.                                                     | ```python<br>from nltk.tokenize import word_tokenize<br>text = \"Natural language processing!\"<br>words = word_tokenize(text)<br>print(words)  # ['Natural', 'language', 'processing', '!']``` |\n",
    "|                                      | Sentence Tokenization                | `sent_tokenize(text)`                          | Splits text into a list of sentences.                                                 | ```python<br>from nltk.tokenize import sent_tokenize<br>text = \"Hello world. How are you?\"<br>sentences = sent_tokenize(text)<br>print(sentences)  # ['Hello world.', 'How are you?']```        |\n",
    "| **Stop Words Removal**               | Stopwords                            | `stopwords.words('language')`                 | Retrieves a list of stop words for a specified language.                             | ```python<br>from nltk.corpus import stopwords<br>stop_words = set(stopwords.words('english'))<br>print(stop_words)  # set of English stop words```                |\n",
    "| **Stemming**                         | Porter Stemmer                      | `PorterStemmer()`                              | Reduces words to their root form using the Porter stemming algorithm.                  | ```python<br>from nltk.stem import PorterStemmer<br>ps = PorterStemmer()<br>stemmed_word = ps.stem(\"running\")<br>print(stemmed_word)  # 'run'```                          |\n",
    "| **Lemmatization**                    | WordNet Lemmatizer                  | `WordNetLemmatizer()`                         | Reduces words to their base form, considering context.                                 | ```python<br>from nltk.stem import WordNetLemmatizer<br>lemmatizer = WordNetLemmatizer()<br>lemmatized_word = lemmatizer.lemmatize(\"better\", pos='a')<br>print(lemmatized_word)  # 'good'``` |\n",
    "| **Part-of-Speech Tagging (POS)**    | POS Tagging                         | `pos_tag(tokens)`                             | Tags a list of words with their corresponding parts of speech.                        | ```python<br>from nltk import pos_tag<br>tokens = word_tokenize(\"Python is fun\")<br>pos_tags = pos_tag(tokens)<br>print(pos_tags)  # [('Python', 'NNP'), ('is', 'VBZ'), ('fun', 'NN')]```  |\n",
    "| **Named Entity Recognition (NER)**   | NER Chunking                        | `ne_chunk(pos_tags)`                          | Identifies and classifies entities in text, such as names and organizations.          | ```python<br>from nltk import ne_chunk<br>ner_tree = ne_chunk(pos_tags)<br>print(ner_tree)  # Output: Tree with named entities```                                   |\n",
    "| **Text Classification**              | Naive Bayes Classifier              | `NaiveBayesClassifier.train(train_data)`      | Trains a Naive Bayes classifier using labeled training data.                          | ```python<br>from nltk.classify import NaiveBayesClassifier<br>train_data = [({'word': 'happy'}, 'positive'), ({'word': 'sad'}, 'negative')]<br>classifier = NaiveBayesClassifier.train(train_data)<br>print(classifier.classify({'word': 'happy'}))  # 'positive'```|\n",
    "| **Corpora Access**                   | Accessing Corpora                   | `nltk.corpus.corpora_name.words()`            | Provides access to various linguistic datasets.                                        | ```python<br>from nltk.corpus import movie_reviews<br>print(movie_reviews.words()[:10])  # Output: First 10 words in the movie reviews corpus```                        |\n",
    "| **Text Parsing**                     | Recursive Descent Parser             | `RecursiveDescentParser(grammar)`             | Analyzes the grammatical structure of a sentence.                                      | ```python<br>from nltk import CFG, RecursiveDescentParser<br>grammar = CFG.fromstring(\"S -> NP VP; NP -> Det N; VP -> V NP; Det -> 'the'; N -> 'dog' | 'cat'; V -> 'chased' | 'sat'\")<br>parser = RecursiveDescentParser(grammar)<br>for tree in parser.parse(\"the dog chased the cat\".split()): print(tree)```|\n",
    "| **Frequency Distribution**           | FreqDist                            | `FreqDist(tokens)`                             | Computes the frequency distribution of tokens.                                        | ```python<br>from nltk import FreqDist<br>import matplotlib.pyplot as plt<br>words = [\"dog\", \"cat\", \"dog\", \"cat\", \"cat\", \"mouse\"]<br>fdist = FreqDist(words)<br>fdist.plot(30, cumulative=False)<br>plt.show()```|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The End\n"
     ]
    }
   ],
   "source": [
    "print(\"The End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
